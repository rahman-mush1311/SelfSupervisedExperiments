{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0cc6dc-6584-4eee-ba4d-f6ac790a643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2ca2f1-292a-400d-91f1-48743ed790ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61729937-ea67-407b-a765-6aa16cb077aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    # Ensure images are converted to tensors\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6259151-a54f-418e-9cb8-b947b6dc3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=r\"C:\\Users\\mushfika_rahman1\\Documents\\Research\\research-summer-2024\\LIMUC\\train_and_validation_sets\",transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17acff46-8bea-4df1-bfd7-a66229ef6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_images, batch_labels in dataloader:\n",
    "    # Print the type of batch_images and batch_labels\n",
    "    print(f\"Type of batch_images: {type(batch_images)}\")\n",
    "    print(f\"Type of batch_labels: {type(batch_labels)}\")\n",
    "\n",
    "    # Print the shape of batch_images to get the batch size and image dimensions\n",
    "    print(f\"Batch image tensor size: {batch_images.size()}\")  # Should print [batch_size, channels, height, width]\n",
    "    print(f\"Batch labels: {batch_labels}\")\n",
    "\n",
    "    # Optionally, print the batch size only\n",
    "    batch_size = batch_images.size(0)\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Optionally, you can also check if the elements in the batch are tensors\n",
    "    if isinstance(batch_images, torch.Tensor):\n",
    "        print(\"Batch images are tensors.\")\n",
    "    else:\n",
    "        print(\"Batch images are not tensors.\")\n",
    "    \n",
    "    # You only need to check the first batch to confirm\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a9d2800-372c-4d80-ab24-9a000d084b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(x):\n",
    "    angles = [0, 90, 180, 270]\n",
    "    rotated_images=[]\n",
    "    rotated_labels=[]\n",
    "    for angle in angles:\n",
    "        rotated_img = TF.rotate(x,angle)\n",
    "        rotated_images.append(rotated_img)\n",
    "        rotated_labels.append(angles.index(angle))\n",
    "        #rotated_labels.append(angle)\n",
    "    return rotated_images,rotated_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2abc570-615c-46f5-a0b6-f0b4f849c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRotatedDataset(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.dataset = original_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)*4\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        original_index = index // 4 \n",
    "        angle_index = index % 4 \n",
    "        \n",
    "        img, _ = self.dataset[original_index]\n",
    "        rotated_img, rotated_labels = rotate_image(img)\n",
    "        return rotated_img[angle_index], rotated_labels[angle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9173c6c-1a7a-4bc2-aa3d-f75ed931b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dataset = datasets.ImageFolder(root=r\"C:\\Users\\mushfika_rahman1\\Documents\\Research\\research-summer-2024\\LIMUC\\subset\",transform=transform)\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d2a4c7-93e4-4431-b788-24fafdd34bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_dataset = CustomRotatedDataset(subset_dataset)\n",
    "rotated_loader = DataLoader(rotated_dataset, batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9630767-dd86-46a1-9467-e1413811ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 4)  # 4 classes for 0째, 90째, 180째, 270째\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5289ea-e8e8-4be7-9369-201fa395cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f35732-a4d5-46ea-9ddc-da5fe2aba1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'resnet18_model_weights_LIMUC'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34113fd-cbfc-45eb-8d1f-3be32379f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3692\n",
      "Epoch [2/10], Loss: 0.0500\n",
      "Epoch [3/10], Loss: 0.0089\n",
      "Epoch [4/10], Loss: 0.0031\n",
      "Epoch [5/10], Loss: 0.0016\n",
      "Model weights saved for epoch 5 at resnet18_model_weights_LIMUC\\model_epoch_5.pth\n",
      "Epoch [6/10], Loss: 0.0103\n",
      "Epoch [7/10], Loss: 0.0237\n",
      "Epoch [8/10], Loss: 0.0139\n",
      "Epoch [9/10], Loss: 0.1017\n",
      "Epoch [10/10], Loss: 0.0229\n",
      "Model weights saved for epoch 10 at resnet18_model_weights_LIMUC\\model_epoch_10.pth\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for images, labels in rotated_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(rotated_loader):.4f}')\n",
    "    if (epoch+1)%5==0:\n",
    "        save_path = os.path.join(save_dir, f'model_epoch_{epoch + 1}.pth')\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f'Model weights saved for epoch {epoch + 1} at {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5af16b-b377-403d-84af-1dccde6ea9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(num_ftrs, len(subset_dataset.classes))\n",
    "model.load_state_dict(torch.load('resnet18_model_weights_LIMUC/model_epoch_5.pth'))  # Replace with the correct path\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0705c322-22a3-4b9a-85df-76b30820d1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.7125\n",
      "Epoch [2/10], Loss: 1.3512\n",
      "Epoch [3/10], Loss: 1.0795\n",
      "Epoch [4/10], Loss: 0.7704\n",
      "Epoch [5/10], Loss: 0.5458\n",
      "Epoch [6/10], Loss: 0.4477\n",
      "Epoch [7/10], Loss: 0.2926\n",
      "Epoch [8/10], Loss: 0.1432\n",
      "Epoch [9/10], Loss: 0.0928\n",
      "Epoch [10/10], Loss: 0.1826\n",
      "Model fine-tuned on subset dataset and weights saved.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(subset_loader):.4f}')\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), 'resnet18_finetuned_subset.pth')\n",
    "print(\"Model fine-tuned on subset dataset and weights saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e90f3f-0734-428e-92a4-1e26c8840c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9250\n",
      "F1 Score: 0.9261\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in subset_loader:  \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71736d74-8d4e-4f09-b149-7d0b1ac4222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=r\"C:\\Users\\mushfika_rahman1\\Documents\\Research\\research-summer-2024\\LIMUC\\test_set\", transform=test_transform)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f44374b-a7b5-45ff-8d9f-f402e8f969b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2888\n",
      "F1 Score on test set: 0.2828\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:  \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Accuracy on test set: {accuracy:.4f}')\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f'F1 Score on test set: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b1704-b210-4fe8-b8c4-a6709503d3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpu",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
